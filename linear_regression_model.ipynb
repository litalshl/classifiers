{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logistic Regression - Respondent 1 & 3 </h3>\n",
    "\n",
    "<h2>Pre-processing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Anger  Contempt   Disgust      Fear       Joy   Sadness  Surprise  \\\n",
      "3705  0.125566  0.136948  0.021518  0.116770  0.041248  0.112264  0.059114   \n",
      "3706  0.125566  0.137617  0.021531  0.132963  0.040812  0.112249  0.067522   \n",
      "3707  0.125566  0.137096  0.021582  0.169795  0.040925  0.111568  0.087434   \n",
      "3708  0.125566  0.136868  0.021714  0.218535  0.041063  0.111219  0.114075   \n",
      "3709  0.125566  0.137239  0.021806  0.256706  0.040785  0.110944  0.135015   \n",
      "\n",
      "      Engagement  Valence  Sentimentality  Confusion Cognitive Task Screens  \n",
      "3705    0.328766      0.0        0.976782   0.036977        Explore     Map  \n",
      "3706    0.328766      0.0        0.911952   0.034095        Explore     Map  \n",
      "3707    0.328766      0.0        0.831274   0.026797        Explore     Map  \n",
      "3708    0.328766      0.0        0.829263   0.019318        Explore     Map  \n",
      "3709    0.328766      0.0        0.773583   0.014509        Explore     Map  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '001-b45731a0_emotions_with_cognitive_tesk_and_screen.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Load the second CSV file\n",
    "new_file_path = '003-91dc3428_emotions_with_cognitive_tesk_and_screen.csv'\n",
    "new_data = pd.read_csv(new_file_path)\n",
    "\n",
    "# Merge the two dataframes\n",
    "data = pd.concat([data, new_data])\n",
    "\n",
    "# Drop the Timestamp column\n",
    "data = data.drop('Timestamp', axis=1)\n",
    "\n",
    "# Drop rows with NaN values in the target column\n",
    "data = data.dropna(subset=['Cognitive Task'])\n",
    "print(data.head())\n",
    "\n",
    "# Select only relevant columns\n",
    "selected_features = ['Anger', 'Contempt', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise', 'Confusion', 'Cognitive Task', 'Screens']\n",
    "data = data[selected_features]\n",
    "\n",
    "# Define the categories from the 'screens' column you want to include as features\n",
    "selected_screens = ['Filter', 'Supporting material', 'Map', 'Statistics']\n",
    "\n",
    "# Identify numeric and categorical features, excluding the target variable\n",
    "all_columns = data.columns.tolist()\n",
    "target_column = 'Cognitive Task'\n",
    "all_columns.remove(target_column)\n",
    "numeric_features = data[all_columns].select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = ['Screens']\n",
    "\n",
    "# Exclude the target column from the numeric features\n",
    "numeric_features = numeric_features.drop('Cognitive Task', errors='ignore')\n",
    "\n",
    "# Define the preprocessing steps for numeric and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(categories=[selected_screens], handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "# Combine the preprocessing steps into a single preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Preprocess the data\n",
    "y = data['Cognitive Task']\n",
    "X = data.drop('Cognitive Task', axis=1)\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Combine the numeric and transformed categorical feature names\n",
    "all_feature_names = list(numeric_features) + selected_screens\n",
    "\n",
    "# Convert the preprocessed data back to a DataFrame\n",
    "X_preprocessed = pd.DataFrame(X_preprocessed, columns=all_feature_names)\n",
    "\n",
    "# Reset the index of y\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Logistic Regression Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create an instance of the LogisticRegression class\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score: 0.4864244869053821\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Use 5-fold cross-validation to evaluate the model\n",
    "cv_scores = cross_val_score(log_reg, X_preprocessed, y, cv=5)\n",
    "print(\"Average cross-validation score:\", np.mean(cv_scores))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
